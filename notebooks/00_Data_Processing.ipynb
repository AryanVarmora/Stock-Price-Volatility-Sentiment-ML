{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing: From Raw Files to Final Dataset\n",
    "\n",
    "This notebook processes raw stock data and creates the final dataset for machine learning.\n",
    "\n",
    "**Steps:**\n",
    "1. Load and clean raw stock files (AAPL, TSLA, MSFT)\n",
    "2. Calculate technical indicators\n",
    "3. Add sentiment features\n",
    "4. Combine everything into final_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STOCK DATA PROCESSING PIPELINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STOCK DATA PROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Functions defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def process_stock(filepath, ticker_name):\n",
    "    \"\"\"Process a single stock file with all technical indicators\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {ticker_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load raw data\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"   Loaded: {len(df)} rows\")\n",
    "    \n",
    "    # Clean data - remove $ symbols\n",
    "    df['Close'] = df['Close/Last'].str.replace('$', '').astype(float)\n",
    "    df['Open'] = df['Open'].str.replace('$', '').astype(float)\n",
    "    df['High'] = df['High'].str.replace('$', '').astype(float)\n",
    "    df['Low'] = df['Low'].str.replace('$', '').astype(float)\n",
    "    \n",
    "    # Drop the old column\n",
    "    df = df.drop('Close/Last', axis=1)\n",
    "    \n",
    "    # Convert date and sort\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    print(\"   Calculating technical indicators...\")\n",
    "    \n",
    "    # RSI\n",
    "    df['RSI'] = calculate_rsi(df['Close'])\n",
    "    \n",
    "    # EMA\n",
    "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    ema_fast = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_slow = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = ema_fast - ema_slow\n",
    "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    sma = df['Close'].rolling(window=20).mean()\n",
    "    std = df['Close'].rolling(window=20).std()\n",
    "    df['BB_Upper'] = sma + (std * 2)\n",
    "    df['BB_Middle'] = sma\n",
    "    df['BB_Lower'] = sma - (std * 2)\n",
    "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "    \n",
    "    # Price metrics\n",
    "    df['Price_Change'] = df['Close'].pct_change()\n",
    "    df['Price_Direction'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "    df['Volatility_20'] = df['Close'].pct_change().rolling(window=20).std()\n",
    "    df['Daily_Return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['High_Low_Range'] = (df['High'] - df['Low']) / df['Low']\n",
    "    \n",
    "    # Add ticker column\n",
    "    df['Ticker'] = ticker_name\n",
    "    \n",
    "    # Remove NaN rows (from rolling calculations)\n",
    "    original_len = len(df)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"   Removed {original_len - len(df)} NaN rows\")\n",
    "    print(f\"   Final: {len(df)} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✅ Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: PROCESSING INDIVIDUAL STOCKS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Processing AAPL\n",
      "============================================================\n",
      "   Loaded: 1255 rows\n",
      "   Date range: 2020-12-02 to 2025-12-01\n",
      "   Calculating technical indicators...\n",
      "   Removed 20 NaN rows\n",
      "   Final: 1235 rows with 22 columns\n",
      "\n",
      "============================================================\n",
      "Processing TSLA\n",
      "============================================================\n",
      "   Loaded: 1255 rows\n",
      "   Date range: 2020-12-02 to 2025-12-01\n",
      "   Calculating technical indicators...\n",
      "   Removed 20 NaN rows\n",
      "   Final: 1235 rows with 22 columns\n",
      "\n",
      "============================================================\n",
      "Processing MSFT\n",
      "============================================================\n",
      "   Loaded: 1255 rows\n",
      "   Date range: 2020-12-02 to 2025-12-01\n",
      "   Calculating technical indicators...\n",
      "   Removed 20 NaN rows\n",
      "   Final: 1235 rows with 22 columns\n",
      "\n",
      "✅ All stocks processed successfully!\n",
      "\n",
      "================================================================================\n",
      "SAVING COMBINED STOCKS FILE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAVING COMBINED STOCKS FILE\n",
      "================================================================================\n",
      "✅ Saved: all_stocks_combined.csv\n",
      "   Location: /Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/all_stocks_combined.csv\n",
      "   Rows: 3,705\n",
      "   Columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Process each stock\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: PROCESSING INDIVIDUAL STOCKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# UPDATE THESE PATHS TO YOUR ACTUAL FILE LOCATIONS\n",
    "aapl = process_stock('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/AAPL.csv', 'AAPL')\n",
    "tsla = process_stock('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/TSLA.csv', 'TSLA')\n",
    "msft = process_stock('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/MSFT.csv', 'MSFT')\n",
    "\n",
    "print(\"\\n✅ All stocks processed successfully!\")\n",
    "\n",
    "\n",
    "# Save combined stocks (before adding sentiment)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING COMBINED STOCKS FILE\")\n",
    "print(\"=\"*80)\n",
    "# Save combined stocks (before adding sentiment)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING COMBINED STOCKS FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save in the main data folder (no 'processed' subfolder needed)\n",
    "combined_stocks_path = '/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/all_stocks_combined.csv'\n",
    "all_stocks.to_csv(combined_stocks_path, index=False)\n",
    "\n",
    "print(f\"✅ Saved: all_stocks_combined.csv\")\n",
    "print(f\"   Location: {combined_stocks_path}\")\n",
    "print(f\"   Rows: {len(all_stocks):,}\")\n",
    "print(f\"   Columns: {len(all_stocks.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: COMBINING ALL STOCKS\n",
      "================================================================================\n",
      "\n",
      "Combined dataset:\n",
      "   Total rows: 3,705\n",
      "   AAPL: 1,235 rows\n",
      "   TSLA: 1,235 rows\n",
      "   MSFT: 1,235 rows\n",
      "\n",
      "Sample of combined data:\n",
      "        Date Ticker   Close        RSI  Price_Direction\n",
      "0 2020-12-31   AAPL  132.69  67.870651                0\n",
      "1 2021-01-04   AAPL  129.41  62.114919                1\n",
      "2 2021-01-05   AAPL  131.01  65.455459                0\n",
      "3 2021-01-06   AAPL  126.60  47.728080                1\n",
      "4 2021-01-07   AAPL  130.92  54.796422                1\n",
      "5 2021-01-08   AAPL  132.05  55.128598                0\n",
      "6 2021-01-11   AAPL  128.98  53.451091                0\n",
      "7 2021-01-12   AAPL  128.80  50.882626                1\n",
      "8 2021-01-13   AAPL  130.89  48.389196                0\n",
      "9 2021-01-14   AAPL  128.91  46.775716                0\n",
      "\n",
      "✅ Stocks combined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Combine all stocks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: COMBINING ALL STOCKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_stocks = pd.concat([aapl, tsla, msft], ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"   Total rows: {len(all_stocks):,}\")\n",
    "print(f\"   AAPL: {len(aapl):,} rows\")\n",
    "print(f\"   TSLA: {len(tsla):,} rows\")\n",
    "print(f\"   MSFT: {len(msft):,} rows\")\n",
    "\n",
    "print(\"\\nSample of combined data:\")\n",
    "print(all_stocks[['Date', 'Ticker', 'Close', 'RSI', 'Price_Direction']].head(10))\n",
    "\n",
    "print(\"\\n✅ Stocks combined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: ADDING SENTIMENT FEATURES\n",
      "================================================================================\n",
      "Sentiment data loaded: 108,751 sentences\n",
      "\n",
      "Sentiment Analysis:\n",
      "   Positive: 55,725 (51.2%)\n",
      "   Negative: 53,026 (48.8%)\n",
      "   Overall score: 0.512\n",
      "\n",
      "✅ Added 4 sentiment features\n"
     ]
    }
   ],
   "source": [
    "# Add sentiment features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: ADDING SENTIMENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load sentiment data\n",
    "# UPDATE THIS PATH TO YOUR SENTIMENT FILE\n",
    "sentiment_df = pd.read_csv('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/Sentiment_Stock_data.csv')\n",
    "\n",
    "print(f\"Sentiment data loaded: {len(sentiment_df):,} sentences\")\n",
    "\n",
    "# Calculate sentiment statistics\n",
    "total_sentences = len(sentiment_df)\n",
    "positive_sentences = (sentiment_df['Sentiment'] == 1).sum()\n",
    "negative_sentences = (sentiment_df['Sentiment'] == 0).sum()\n",
    "\n",
    "sentiment_score = positive_sentences / total_sentences\n",
    "\n",
    "print(f\"\\nSentiment Analysis:\")\n",
    "print(f\"   Positive: {positive_sentences:,} ({positive_sentences/total_sentences*100:.1f}%)\")\n",
    "print(f\"   Negative: {negative_sentences:,} ({negative_sentences/total_sentences*100:.1f}%)\")\n",
    "print(f\"   Overall score: {sentiment_score:.3f}\")\n",
    "\n",
    "# Add sentiment features to stock data\n",
    "all_stocks['Sentiment_Score'] = sentiment_score\n",
    "all_stocks['Sentiment_Positive_Ratio'] = positive_sentences / total_sentences\n",
    "all_stocks['Sentiment_Negative_Ratio'] = negative_sentences / total_sentences\n",
    "\n",
    "# Add simulated daily sentiment variation\n",
    "np.random.seed(42)\n",
    "all_stocks['Daily_Sentiment'] = np.random.normal(sentiment_score, 0.1, len(all_stocks))\n",
    "all_stocks['Daily_Sentiment'] = all_stocks['Daily_Sentiment'].clip(0, 1)\n",
    "\n",
    "print(f\"\\n✅ Added 4 sentiment features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: ADDING SENTIMENT FEATURES FROM ALL FILES\n",
      "================================================================================\n",
      "\n",
      "[File 1] Loading Sentiment_Stock_data.csv...\n",
      "   Loaded: 108,751 sentences\n",
      "   Positive: 55,725 (51.2%)\n",
      "   Negative: 53,026 (48.8%)\n",
      "   Score: 0.512\n",
      "\n",
      "[File 2] Loading all-data__1_.csv...\n",
      "   Loaded: 4,845 sentences\n",
      "   Distribution:\n",
      "      neutral: 2,878 (59.4%)\n",
      "      positive: 1,363 (28.1%)\n",
      "      negative: 604 (12.5%)\n",
      "   Score: 0.281\n",
      "\n",
      "[File 3] Loading Data.csv...\n",
      "   Loaded: 4,101 rows\n",
      "   Positive (1): 2,166 (52.8%)\n",
      "   Negative (0): 1,935 (47.2%)\n",
      "   Score: 0.528\n",
      "   Date range: 2000-01-03 to 2016-07-01\n",
      "\n",
      "============================================================\n",
      "COMBINED SENTIMENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total sentiment data points: 117,697\n",
      "   From Sentiment_Stock_data.csv: 108,751\n",
      "   From all-data__1_.csv: 4,845\n",
      "   From Data.csv: 4,101\n",
      "\n",
      "Individual sentiment scores:\n",
      "   File 1 score: 0.512\n",
      "   File 2 score: 0.281\n",
      "   File 3 score: 0.528\n",
      "\n",
      "Weighted average sentiment: 0.503\n",
      "\n",
      "============================================================\n",
      "ADDING SENTIMENT FEATURES TO STOCK DATA\n",
      "============================================================\n",
      "\n",
      "✅ Added 8 sentiment features to all 3,705 rows\n",
      "\n",
      "Sentiment features added:\n",
      "   1. Sentiment_Score (weighted average)\n",
      "   2. Sentiment_File1_Score\n",
      "   3. Sentiment_File2_Score\n",
      "   4. Sentiment_File3_Score\n",
      "   5. Sentiment_Positive_Ratio\n",
      "   6. Sentiment_Negative_Ratio\n",
      "   7. Sentiment_Neutral_Ratio\n",
      "   8. Daily_Sentiment (with variation)\n"
     ]
    }
   ],
   "source": [
    "# Add sentiment features from ALL sentiment files\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: ADDING SENTIMENT FEATURES FROM ALL FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# === FILE 1: Sentiment_Stock_data.csv ===\n",
    "print(\"\\n[File 1] Loading Sentiment_Stock_data.csv...\")\n",
    "sentiment_df1 = pd.read_csv('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/Sentiment_Stock_data.csv')\n",
    "print(f\"   Loaded: {len(sentiment_df1):,} sentences\")\n",
    "\n",
    "total_1 = len(sentiment_df1)\n",
    "positive_1 = (sentiment_df1['Sentiment'] == 1).sum()\n",
    "negative_1 = (sentiment_df1['Sentiment'] == 0).sum()\n",
    "sentiment_score_1 = positive_1 / total_1\n",
    "\n",
    "print(f\"   Positive: {positive_1:,} ({positive_1/total_1*100:.1f}%)\")\n",
    "print(f\"   Negative: {negative_1:,} ({negative_1/total_1*100:.1f}%)\")\n",
    "print(f\"   Score: {sentiment_score_1:.3f}\")\n",
    "\n",
    "\n",
    "# === FILE 2: all-data__1_.csv ===\n",
    "print(\"\\n[File 2] Loading all-data__1_.csv...\")\n",
    "sentiment_df2 = pd.read_csv('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/all-data.csv', encoding='latin1')\n",
    "print(f\"   Loaded: {len(sentiment_df2):,} sentences\")\n",
    "\n",
    "# This file has 'neutral', 'positive', 'negative' as labels\n",
    "# The first column contains the sentiment label\n",
    "sentiment_col = sentiment_df2.columns[0]  # Get first column name\n",
    "sentiment_counts = sentiment_df2[sentiment_col].value_counts()\n",
    "\n",
    "print(f\"   Distribution:\")\n",
    "for label, count in sentiment_counts.items():\n",
    "    print(f\"      {label}: {count:,} ({count/len(sentiment_df2)*100:.1f}%)\")\n",
    "\n",
    "# Calculate sentiment score (positive ratio)\n",
    "positive_2 = sentiment_counts.get('positive', 0)\n",
    "negative_2 = sentiment_counts.get('negative', 0)\n",
    "neutral_2 = sentiment_counts.get('neutral', 0)\n",
    "total_2 = len(sentiment_df2)\n",
    "sentiment_score_2 = positive_2 / total_2 if total_2 > 0 else 0.5\n",
    "\n",
    "print(f\"   Score: {sentiment_score_2:.3f}\")\n",
    "\n",
    "\n",
    "# === FILE 3: Data.csv ===\n",
    "print(\"\\n[File 3] Loading Data.csv...\")\n",
    "sentiment_df3 = pd.read_csv('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/Data.csv', encoding='latin1')\n",
    "print(f\"   Loaded: {len(sentiment_df3):,} rows\")\n",
    "\n",
    "# This file has news headlines with 'Label' column (0/1)\n",
    "total_3 = len(sentiment_df3)\n",
    "positive_3 = (sentiment_df3['Label'] == 1).sum()\n",
    "negative_3 = (sentiment_df3['Label'] == 0).sum()\n",
    "sentiment_score_3 = positive_3 / total_3\n",
    "\n",
    "print(f\"   Positive (1): {positive_3:,} ({positive_3/total_3*100:.1f}%)\")\n",
    "print(f\"   Negative (0): {negative_3:,} ({negative_3/total_3*100:.1f}%)\")\n",
    "print(f\"   Score: {sentiment_score_3:.3f}\")\n",
    "print(f\"   Date range: {sentiment_df3['Date'].min()} to {sentiment_df3['Date'].max()}\")\n",
    "\n",
    "\n",
    "# === COMBINE ALL SENTIMENT SCORES ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINED SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate weighted average sentiment\n",
    "total_sentences = total_1 + total_2 + total_3\n",
    "weighted_sentiment = (\n",
    "    (sentiment_score_1 * total_1 + \n",
    "     sentiment_score_2 * total_2 + \n",
    "     sentiment_score_3 * total_3) / total_sentences\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal sentiment data points: {total_sentences:,}\")\n",
    "print(f\"   From Sentiment_Stock_data.csv: {total_1:,}\")\n",
    "print(f\"   From all-data__1_.csv: {total_2:,}\")\n",
    "print(f\"   From Data.csv: {total_3:,}\")\n",
    "\n",
    "print(f\"\\nIndividual sentiment scores:\")\n",
    "print(f\"   File 1 score: {sentiment_score_1:.3f}\")\n",
    "print(f\"   File 2 score: {sentiment_score_2:.3f}\")\n",
    "print(f\"   File 3 score: {sentiment_score_3:.3f}\")\n",
    "\n",
    "print(f\"\\nWeighted average sentiment: {weighted_sentiment:.3f}\")\n",
    "\n",
    "\n",
    "# === ADD FEATURES TO STOCK DATA ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDING SENTIMENT FEATURES TO STOCK DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add overall sentiment features\n",
    "all_stocks['Sentiment_Score'] = weighted_sentiment\n",
    "all_stocks['Sentiment_File1_Score'] = sentiment_score_1\n",
    "all_stocks['Sentiment_File2_Score'] = sentiment_score_2\n",
    "all_stocks['Sentiment_File3_Score'] = sentiment_score_3\n",
    "\n",
    "# Calculate positive/negative ratios from all files\n",
    "total_positive = positive_1 + positive_2 + positive_3\n",
    "total_negative = negative_1 + negative_2 + negative_3\n",
    "total_neutral = neutral_2  # Only File 2 has neutral\n",
    "\n",
    "all_stocks['Sentiment_Positive_Ratio'] = total_positive / total_sentences\n",
    "all_stocks['Sentiment_Negative_Ratio'] = total_negative / total_sentences\n",
    "all_stocks['Sentiment_Neutral_Ratio'] = total_neutral / total_sentences\n",
    "\n",
    "# Add simulated daily sentiment variation\n",
    "np.random.seed(42)\n",
    "all_stocks['Daily_Sentiment'] = np.random.normal(weighted_sentiment, 0.1, len(all_stocks))\n",
    "all_stocks['Daily_Sentiment'] = all_stocks['Daily_Sentiment'].clip(0, 1)\n",
    "\n",
    "print(f\"\\n✅ Added 8 sentiment features to all {len(all_stocks):,} rows\")\n",
    "print(\"\\nSentiment features added:\")\n",
    "print(\"   1. Sentiment_Score (weighted average)\")\n",
    "print(\"   2. Sentiment_File1_Score\")\n",
    "print(\"   3. Sentiment_File2_Score\")\n",
    "print(\"   4. Sentiment_File3_Score\")\n",
    "print(\"   5. Sentiment_Positive_Ratio\")\n",
    "print(\"   6. Sentiment_Negative_Ratio\")\n",
    "print(\"   7. Sentiment_Neutral_Ratio\")\n",
    "print(\"   8. Daily_Sentiment (with variation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CREATING FINAL DATASET\n",
      "================================================================================\n",
      "Before final NaN check: 3705 rows\n",
      "After final NaN check: 3705 rows\n",
      "\n",
      "✅ FINAL DATASET SAVED!\n",
      "   Location: /Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/final_dataset.csv\n",
      "   Rows: 3,705\n",
      "   Columns: 26\n"
     ]
    }
   ],
   "source": [
    "# Create final dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: CREATING FINAL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select and order columns\n",
    "feature_columns = [\n",
    "    'Date', 'Ticker',\n",
    "    # Price features\n",
    "    'Close', 'Open', 'High', 'Low', 'Volume',\n",
    "    # Technical indicators\n",
    "    'RSI', 'EMA_20', 'EMA_50', \n",
    "    'MACD', 'MACD_Signal', 'MACD_Hist',\n",
    "    'BB_Upper', 'BB_Middle', 'BB_Lower', 'BB_Width',\n",
    "    # Price metrics\n",
    "    'Price_Change', 'Volatility_20', 'Daily_Return', 'High_Low_Range',\n",
    "    # Sentiment features\n",
    "    'Sentiment_Score', 'Sentiment_Positive_Ratio', \n",
    "    'Sentiment_Negative_Ratio', 'Daily_Sentiment',\n",
    "    # Target\n",
    "    'Price_Direction'\n",
    "]\n",
    "\n",
    "final_df = all_stocks[feature_columns].copy()\n",
    "\n",
    "# Final check for NaN\n",
    "print(f\"Before final NaN check: {len(final_df)} rows\")\n",
    "final_df = final_df.dropna()\n",
    "print(f\"After final NaN check: {len(final_df)} rows\")\n",
    "\n",
    "# Save final dataset\n",
    "# UPDATE THIS PATH\n",
    "output_path = '/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/final_dataset.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ FINAL DATASET SAVED!\")\n",
    "print(f\"   Location: {output_path}\")\n",
    "print(f\"   Rows: {len(final_df):,}\")\n",
    "print(f\"   Columns: {len(final_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
