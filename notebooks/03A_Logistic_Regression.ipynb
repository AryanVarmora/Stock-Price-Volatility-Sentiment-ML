{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File 3A: Logistic Regression Model\n",
    "## Stock Price Volatility & Sentiment ML Project\n",
    "\n",
    "**Purpose:** Train and evaluate Logistic Regression model\n",
    "\n",
    "**What is Logistic Regression?**\n",
    "- Simple, interpretable classification model\n",
    "- Good baseline for binary classification (Up/Down)\n",
    "- Fast to train\n",
    "- Works well with linearly separable data\n",
    "\n",
    "**We'll train:**\n",
    "- Model with full features (23 features)\n",
    "- Model with PCA features (7 components)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('LOADING PREPROCESSED DATA')\n",
    "print('='*70)\n",
    "\n",
    "# UPDATE THIS PATH\n",
    "processed_dir = '/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/data/processed'\n",
    "\n",
    "# Load training and testing data\n",
    "X_train = np.load(f'{processed_dir}/X_train_scaled.npy')\n",
    "X_test = np.load(f'{processed_dir}/X_test_scaled.npy')\n",
    "X_train_pca = np.load(f'{processed_dir}/X_train_pca.npy')\n",
    "X_test_pca = np.load(f'{processed_dir}/X_test_pca.npy')\n",
    "y_train = np.load(f'{processed_dir}/y_train.npy')\n",
    "y_test = np.load(f'{processed_dir}/y_test.npy')\n",
    "\n",
    "print('\\n‚úÖ Data loaded successfully!')\n",
    "print(f'\\nData shapes:')\n",
    "print(f'   X_train (full): {X_train.shape}')\n",
    "print(f'   X_test (full): {X_test.shape}')\n",
    "print(f'   X_train (PCA): {X_train_pca.shape}')\n",
    "print(f'   X_test (PCA): {X_test_pca.shape}')\n",
    "print(f'   y_train: {y_train.shape}')\n",
    "print(f'   y_test: {y_test.shape}')\n",
    "\n",
    "print(f'\\nTarget distribution:')\n",
    "print(f'   Train - Up: {y_train.sum()}/{len(y_train)} ({y_train.mean()*100:.1f}%)')\n",
    "print(f'   Test - Up: {y_test.sum()}/{len(y_test)} ({y_test.mean()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Logistic Regression with Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('TRAINING LOGISTIC REGRESSION - FULL FEATURES')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nTraining with 23 features...')\n",
    "print('Hyperparameters:')\n",
    "print('   - Solver: lbfgs')\n",
    "print('   - Max iterations: 1000')\n",
    "print('   - Random state: 42')\n",
    "\n",
    "# Create and train model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "print('\\n‚úÖ Model trained successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Full Features Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = lr_model.predict(X_train)\n",
    "y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print('='*70)\n",
    "print('RESULTS - FULL FEATURES MODEL')\n",
    "print('='*70)\n",
    "print(f'\\nüìä Training Accuracy: {train_acc*100:.2f}%')\n",
    "print(f'üìä Testing Accuracy: {test_acc*100:.2f}%')\n",
    "\n",
    "# Check overfitting\n",
    "diff = train_acc - test_acc\n",
    "if diff > 0.05:\n",
    "    print(f'\\n‚ö†Ô∏è  Overfitting detected! Difference: {diff*100:.2f}%')\n",
    "elif diff < -0.05:\n",
    "    print(f'\\n‚ö†Ô∏è  Underfitting detected! Difference: {diff*100:.2f}%')\n",
    "else:\n",
    "    print(f'\\n‚úÖ Good fit! Difference: {diff*100:.2f}%')\n",
    "\n",
    "# Check target achievement\n",
    "if test_acc >= 0.55:\n",
    "    print(f'‚úÖ Target accuracy (55%) ACHIEVED!')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è  Below target accuracy (55%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print('\\n' + '='*70)\n",
    "print('DETAILED CLASSIFICATION REPORT')\n",
    "print('='*70)\n",
    "print(classification_report(y_test, y_pred_test, \n",
    "                          target_names=['Down (0)', 'Up (1)'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Down (0)', 'Up (1)'],\n",
    "            yticklabels=['Down (0)', 'Up (1)'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "\n",
    "plt.title(f'Logistic Regression - Confusion Matrix\\nAccuracy: {test_acc*100:.2f}%',\n",
    "         fontsize=14, fontweight='bold', pad=15)\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/visualizations/03A_lr_confusion_matrix.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: 03A_lr_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Logistic Regression with PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('TRAINING LOGISTIC REGRESSION - PCA FEATURES')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nTraining with 7 PCA components...')\n",
    "\n",
    "# Create and train model with PCA features\n",
    "lr_model_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "print('\\n‚úÖ PCA model trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PCA model\n",
    "y_pred_pca_test = lr_model_pca.predict(X_test_pca)\n",
    "test_acc_pca = accuracy_score(y_test, y_pred_pca_test)\n",
    "\n",
    "print('='*70)\n",
    "print('RESULTS - PCA MODEL')\n",
    "print('='*70)\n",
    "print(f'\\nüìä Testing Accuracy (PCA): {test_acc_pca*100:.2f}%')\n",
    "\n",
    "# Compare with full features\n",
    "print(f'\\nüîÑ Comparison:')\n",
    "print(f'   Full features (23): {test_acc*100:.2f}%')\n",
    "print(f'   PCA features (7): {test_acc_pca*100:.2f}%')\n",
    "print(f'   Difference: {(test_acc - test_acc_pca)*100:.2f}%')\n",
    "\n",
    "if test_acc_pca >= test_acc * 0.95:\n",
    "    print(f'\\n‚úÖ PCA maintains good performance with fewer features!')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  PCA significantly reduces performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature Coefficients Analysis\n",
    "\n",
    "Let's see which features are most important for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# Load feature names\n",
    "with open(f'{processed_dir}/feature_names.txt', 'r') as f:\n",
    "    feature_names = [line.strip() for line in f]\n",
    "\n",
    "# Create DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Sort by absolute value\n",
    "coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print('='*70)\n",
    "print('TOP 10 MOST IMPORTANT FEATURES')\n",
    "print('='*70)\n",
    "print(coef_df[['Feature', 'Coefficient']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "top_10 = coef_df.head(10)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_10['Coefficient']]\n",
    "\n",
    "plt.barh(range(len(top_10)), top_10['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(top_10)), top_10['Feature'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 10 Most Important Features\\n(Green = Positive influence, Red = Negative influence)',\n",
    "         fontsize=14, fontweight='bold', pad=15)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/visualizations/03A_lr_feature_importance.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: 03A_lr_feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "y_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', linewidth=2.5, label=f'Logistic Regression (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier', alpha=0.5)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "plt.title('ROC Curve - Logistic Regression', fontsize=16, fontweight='bold', pad=15)\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/visualizations/03A_lr_roc_curve.png',\n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: 03A_lr_roc_curve.png')\n",
    "print(f'\\nüìä AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('SAVING MODELS')\n",
    "print('='*70)\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "models_dir = '/Users/aryan/Desktop/Stock-Price-Volatility-Sentiment-ML/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save both models\n",
    "joblib.dump(lr_model, f'{models_dir}/logistic_regression.pkl')\n",
    "joblib.dump(lr_model_pca, f'{models_dir}/logistic_regression_pca.pkl')\n",
    "\n",
    "print('\\n‚úÖ Models saved successfully!')\n",
    "print(f'   - logistic_regression.pkl')\n",
    "print(f'   - logistic_regression_pca.pkl')\n",
    "print(f'\\nLocation: {models_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
